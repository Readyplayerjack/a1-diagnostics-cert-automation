# Diagnostic Scripts - Quick Start

## ğŸ“‹ EXISTING SCRIPTS INVENTORY

**Found: 26 scripts in `scripts/` directory**
- All functional and ready to use
- Use `dotenv -e .env -- tsx` pattern for environment loading

**Found: 4 test files in `src/**/__tests__/`**
- Unit tests for handlers and services

**Found: 30 MD files (HISTORICAL - IGNORE)**
- These are old audit reports - do not use for current state
- Use LIVE diagnostic scripts instead

---

## ğŸ†• NEW LIVE DIAGNOSTIC SCRIPTS

**Location:** `scripts/diagnostics/`

### Created Scripts:

1. âœ… `test-api-connections.ts` - Test Jifeline, OpenAI, Supabase APIs
2. âœ… `test-real-ticket.ts` - Test full pipeline with real ticket ID
3. âœ… `audit-gpt-prompt.ts` - Show actual GPT prompt being used
4. âœ… `check-validations.ts` - Test registration/mileage validation
5. âœ… `test-error-handling.ts` - Test error handling mechanisms
6. âœ… `run-all.ts` - Run all diagnostics in sequence

---

## ğŸš€ HOW TO RUN DIAGNOSTICS

### Run All Diagnostics:
```bash
npm run diagnostic:all
```

### Run Individual Diagnostics:

**Test API Connections:**
```bash
npm run diagnostic:apis
```
Tests: Jifeline Events API, OpenAI API, Supabase Database & Storage

**Test Real Ticket:**
```bash
npm run diagnostic:ticket <TICKET_ID>
```
Example:
```bash
npm run diagnostic:ticket abc12345-def6-7890-ghij-klmnopqrstuv
```
Tests: Full pipeline with real ticket (fetch, conversation, extraction, validation)

**Audit GPT Prompt:**
```bash
npm run diagnostic:gpt-prompt
```
Shows: Actual prompt being sent to GPT-4o-mini, tests with sample data

**Check Validations:**
```bash
npm run diagnostic:validations
```
Tests: Registration format validation, mileage range validation

**Test Error Handling:**
```bash
npm run diagnostic:errors
```
Tests: 404 handling, malformed responses, timeout/retry/rate limiting status

---

## ğŸ“Š WHAT EACH DIAGNOSTIC DOES

### `diagnostic:apis`
- âœ… Fetches 1 event from Jifeline Events API (real call)
- âœ… Sends test prompt to OpenAI (real call)
- âœ… Queries Supabase database (real query)
- âœ… Checks Supabase storage bucket exists
- **Output:** âœ“/âœ— for each API with details

### `diagnostic:ticket <TICKET_ID>`
- âœ… Fetches ticket from Jifeline
- âœ… Gets conversation text
- âœ… Runs GPT extraction
- âœ… Validates extracted data quality
- âœ… Shows confidence scores
- **Output:** Full extraction results with quality assessment

### `diagnostic:gpt-prompt`
- âœ… Shows EXACT prompt from current code (not old docs)
- âœ… Tests with sample conversation
- âœ… Shows input/output
- âœ… Rates prompt quality (0-10)
- **Output:** Complete prompt + test results + quality score

### `diagnostic:validations`
- âœ… Tests registration validation with 8 test cases
- âœ… Tests mileage validation with 8 test cases
- âœ… Shows which validations exist vs missing
- **Output:** Pass/fail for each test case + missing validations list

### `diagnostic:errors`
- âœ… Tests 404 error handling
- âœ… Tests malformed GPT response handling
- âœ… Checks for timeout handling (currently missing)
- âœ… Checks for retry logic (currently missing)
- âœ… Checks for rate limiting (currently missing)
- **Output:** Status of each error handling mechanism

---

## âš ï¸ IMMEDIATE NEXT STEP

**After scripts are created, run:**

```bash
npm run diagnostic:all
```

This will give you **LIVE results** from the **CURRENT codebase** - no reading old reports, just real API calls and real code execution.

---

## ğŸ“ Notes

- All scripts use `dotenv -e .env` to load environment variables
- All scripts make REAL API calls (not mocks)
- All scripts test CURRENT code (not historical documentation)
- Scripts exit with code 0 on success, 1 on failure

---

**End of Diagnostic Scripts README**
